import torch
from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments, RobertaTokenizer, DataCollatorWithPadding
from datasets import load_from_disk
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, hamming_loss
import numpy as np

# âœ… 1. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ
print("ğŸš€ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
model_path = "./codebert_cwe_multi_label"
model = RobertaForSequenceClassification.from_pretrained(model_path)
tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")

# âœ… íŒ¨ë”© ë¬¸ì œ í•´ê²° (ë°ì´í„°ì…‹ ë‚´ ê¸¸ì´ ì°¨ì´ë¥¼ ë§ì¶°ì¤Œ)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

# âœ… 2. ë°ì´í„°ì…‹ ë¡œë“œ
print("ğŸš€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
test_dataset = load_from_disk("test_dataset")
print(f"âœ… Test ë°ì´í„° ê°œìˆ˜: {len(test_dataset)}")

# âœ… 3. í‰ê°€ í•¨ìˆ˜ ì •ì˜ (ì„¸ë¶€ì ì¸ ì§€í‘œ ì¶”ê°€)
def compute_metrics(pred):
    labels = pred.label_ids  # ì‹¤ì œ ë¼ë²¨ (One-hot Encoding)
    preds = torch.sigmoid(torch.tensor(pred.predictions)) > 0.3 # ì„ê³„ê°’ 0.3 ì´ìƒì„ 1(ì˜ˆì¸¡)ë¡œ ë³€í™˜
    
    # ğŸ”¹ ì •í™•ë„ (Accuracy)
    accuracy = accuracy_score(labels, preds)

    # ğŸ”¹ Precision, Recall, F1-score (Macro, Micro, Weighted)
    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(labels, preds, average="macro", zero_division=0)
    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(labels, preds, average="weighted", zero_division=0)

    # ğŸ”¹ Hamming Loss (ì „ì²´ ì˜ˆì¸¡ ì˜¤ë¥˜ìœ¨)
    hamming = hamming_loss(labels, preds)

    return {
        "ì •í™•ë„(Accuracy)": accuracy,
        "ì •ë°€ë„(Precision, Macro)": precision_macro,
        "ì¬í˜„ìœ¨(Recall, Macro)": recall_macro,
        "F1-ì ìˆ˜(F1-score, Macro)": f1_macro,
        "ì •ë°€ë„(Precision, Weighted)": precision_weighted,
        "ì¬í˜„ìœ¨(Recall, Weighted)": recall_weighted,
        "F1-ì ìˆ˜(F1-score, Weighted)": f1_weighted,
        "Hamming Loss": hamming,
    }

# âœ… 4. í‰ê°€ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./codebert_cwe_multi_label",
    per_device_eval_batch_size=8,
    logging_dir="./logs",
)

trainer = Trainer(
    model=model,
    args=training_args,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
    data_collator=data_collator,  # âœ… íŒ¨ë”© ìë™ ì¡°ì • ì¶”ê°€
)

# âœ… 5. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€
print("ğŸš€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€ ì¤‘...")
test_results = trainer.evaluate()
# âœ… 6. í•µì‹¬ ì§€í‘œë§Œ ì¶œë ¥
print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€ ê²°ê³¼:")
print(f"âœ… ì •í™•ë„(Accuracy): {test_results['eval_ì •í™•ë„(Accuracy)']:.4f}")
print(f"âœ… ì •ë°€ë„(Precision, Macro): {test_results['eval_ì •ë°€ë„(Precision, Macro)']:.4f}")
print(f"âœ… ì¬í˜„ìœ¨(Recall, Macro): {test_results['eval_ì¬í˜„ìœ¨(Recall, Macro)']:.4f}")
print(f"âœ… F1-ì ìˆ˜(F1-score, Macro): {test_results['eval_F1-ì ìˆ˜(F1-score, Macro)']:.4f}")
print(f"âœ… ì •ë°€ë„(Precision, Weighted): {test_results['eval_ì •ë°€ë„(Precision, Weighted)']:.4f}")
print(f"âœ… ì¬í˜„ìœ¨(Recall, Weighted): {test_results['eval_ì¬í˜„ìœ¨(Recall, Weighted)']:.4f}")
print(f"âœ… F1-ì ìˆ˜(F1-score, Weighted): {test_results['eval_F1-ì ìˆ˜(F1-score, Weighted)']:.4f}")
print(f"âœ… Hamming Loss: {test_results['eval_Hamming Loss']:.6f}")
