import torch
from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments, RobertaTokenizer, DataCollatorWithPadding
from datasets import load_from_disk
import pandas as pd

# âœ… 1. CodeBERT ëª¨ë¸ ë¡œë“œ (ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„ë¥˜ ì„¤ì • ì¶”ê°€)
print("ğŸš€ ëª¨ë¸ ë¡œë“œ ì¤‘...")
# train_dataset = load_from_disk("train_dataset")  # âœ… Train ë°ì´í„° ë¡œë“œ
train_dataset = load_from_disk("train_T_dataset")  # âœ… Train ë°ì´í„° ë¡œë“œ
num_labels = len(train_dataset[0]["label"])  # âœ… ë¼ë²¨ ê°œìˆ˜ ê°€ì ¸ì˜¤ê¸°

model = RobertaForSequenceClassification.from_pretrained(
    # "microsoft/codebert-base",
    "neulab/codebert-c",
    num_labels=num_labels,
    problem_type="multi_label_classification"  # ğŸ”¹ ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„ë¥˜ ì„¤ì • ì¶”ê°€
)

print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì´ {num_labels}ê°œì˜ CWEë¥¼ ë™ì‹œì— ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •ë¨.")

# âœ… GPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ í˜„ì¬ í•™ìŠµ ë””ë°”ì´ìŠ¤: {device}")
model.to(device)  # ëª¨ë¸ì„ GPUë¡œ ì´ë™

# âœ… 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
# tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")
tokenizer = RobertaTokenizer.from_pretrained("neulab/codebert-c")

# âœ… 3. ë°ì´í„°ì…‹ ë¡œë“œ (Train / Validation)
print("ğŸš€ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
# valid_dataset = load_from_disk("valid_dataset")
valid_dataset = load_from_disk("valid_T_dataset")

print(f"âœ… Train: {len(train_dataset)}ê°œ, Valid: {len(valid_dataset)}ê°œ")

# âœ… 4. íŒ¨ë”© ë¬¸ì œ í•´ê²° (Data Collator ì¶”ê°€)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# âœ… 5. í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    # output_dir="./codebert_cwe_multi_label",
    output_dir="./codebert_cwe_multi_label_neulab_C",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=5,
    per_device_train_batch_size=8,  # ğŸš€ GPU VRAM ë¶€ì¡±í•˜ë©´ ì¤„ì´ê¸°
    per_device_eval_batch_size=8,
    logging_dir="./logs",
    logging_steps=100,
    learning_rate=2e-5,
    weight_decay=0.01,
    save_total_limit=2,
    load_best_model_at_end=True,
)

# âœ… 6. Trainer ì„¤ì • ë° í•™ìŠµ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    data_collator=data_collator,
)

print("ğŸš€ ë‹¤ì¤‘ ë ˆì´ë¸” í•™ìŠµ ì‹œì‘!")
trainer.train()
print("âœ… í•™ìŠµ ì™„ë£Œ!")

# âœ… 7. ëª¨ë¸ ì €ì¥
# model.save_pretrained("./codebert_cwe_multi_label")
model.save_pretrained("./codebert_cwe_multi_label_neulab_C")
print("âœ… ë‹¤ì¤‘ ë ˆì´ë¸” ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
