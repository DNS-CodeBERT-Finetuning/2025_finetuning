import json
from transformers import RobertaTokenizer

# âœ… CodeBERT í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")

# âœ… JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
# json_path = "cwe_dataset_split.json"
json_path = "cwe_dataset_T_split.json"
with open(json_path, "r", encoding="utf-8") as f:
    data = json.load(f)

# âœ… í† í°í™” ë° ë³€í™˜ëœ ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸
tokenized_data = []
MAX_LENGTH = 512  # CodeBERTì˜ ìµœëŒ€ í† í° ê¸¸ì´
STRIDE = 256  # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° (ê²¹ì¹˜ëŠ” í† í° ìˆ˜)

# âœ… JSON ë°ì´í„° í† í°í™” ì§„í–‰
for i, entry in enumerate(data):
    code_snippet = entry["code_snippet"]
    
    # ğŸ”¹ CodeBERT í† í°í™” ì§„í–‰ (ê¸´ ì½”ë“œ ì²˜ë¦¬ í¬í•¨)
    tokenized = tokenizer(code_snippet, padding=False, truncation=False)

    input_ids = tokenized["input_ids"]
    
    # ğŸ”¹ ì½”ë“œê°€ 512ê°œ ì´í•˜ì´ë©´ ê·¸ëŒ€ë¡œ ì €ì¥
    if len(input_ids) <= MAX_LENGTH:
        tokenized_data.append({
            "input_ids": input_ids,
            "attention_mask": [1] * len(input_ids),
            "label": entry["label"],
            "type": entry["type"],
            "code_snippet": code_snippet  # âœ… ì›ë³¸ ì½”ë“œ ìœ ì§€
        })
    else:
        # ğŸ”¹ 512ê°œ ì´ˆê³¼ ì‹œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
        for start in range(0, len(input_ids), STRIDE):
            end = start + MAX_LENGTH
            chunk = input_ids[start:end]

            # ë§ˆì§€ë§‰ ë¸”ë¡ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ë²„ë¦¬ê¸° (ì˜ˆ: 100ê°œ ë¯¸ë§Œ)
            if len(chunk) < 100:
                continue

            tokenized_data.append({
                "input_ids": chunk,
                "attention_mask": [1] * len(chunk),
                "label": entry["label"],
                "type": entry["type"],
                "code_snippet": code_snippet  # âœ… ì›ë³¸ ì½”ë“œ ìœ ì§€
            })

    # ğŸ”„ ì§„í–‰ ìƒí™© ì¶œë ¥ (1000ê°œë§ˆë‹¤ í•œ ë²ˆì”© ì¶œë ¥)
    if (i + 1) % 1000 == 0:
        print(f"ğŸ”„ {i + 1}ê°œ í† í°í™” ì™„ë£Œ...")

# âœ… ë¼ë²¨ ì¸ë±ì‹±ì„ ìœ„í•´ ê³ ìœ í•œ CWE ë¼ë²¨ ìˆ˜ì§‘
unique_labels = sorted(set(str(entry["label"]) for entry in tokenized_data))  # ëª¨ë“  ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ì •ë ¬
label_map = {label: i for i, label in enumerate(unique_labels)}  # 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì¸ë±ìŠ¤ ë§¤í•‘
reverse_label_map = {v: k for k, v in label_map.items()}  # âœ… ì—­ë§¤í•‘ ìƒì„±

# âœ… ê¸°ì¡´ ë¼ë²¨ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
for entry in tokenized_data:
    entry["label"] = label_map[str(entry["label"])]  # âœ… ì •ìˆ˜ â†’ ë¬¸ìì—´ ë³€í™˜ í›„ ë§¤í•‘

# âœ… í† í°í™”ëœ ë°ì´í„° JSON íŒŒì¼ë¡œ ì €ì¥
# tokenized_json_path = "cwe_dataset_tokenized.json"
tokenized_json_path = "cwe_dataset_T_tokenized.json"

with open(tokenized_json_path, "w", encoding="utf-8") as f:
    json.dump(tokenized_data, f, indent=4)

# âœ… ë¼ë²¨ ë§¤í•‘ JSON ì €ì¥
# with open("label_map.json", "w", encoding="utf-8") as f:
with open("label_T_map.json", "w", encoding="utf-8") as f:
    json.dump(reverse_label_map, f, indent=4)

print(f"âœ… CodeBERT í† í°í™” ì™„ë£Œ! ì´ {len(tokenized_data)}ê°œì˜ ë°ì´í„°ê°€ ì €ì¥ë¨.")
print(f"ğŸ“ JSON íŒŒì¼ ê²½ë¡œ: {tokenized_json_path}")
print(f"âœ… ë¼ë²¨ ë§¤í•‘ ì €ì¥ ì™„ë£Œ! íŒŒì¼: label_T_map.json")
